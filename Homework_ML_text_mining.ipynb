{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ6qMJrS_AoS"
      },
      "source": [
        "# Homework 6: Classification of texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD62FIn9_AoT"
      },
      "source": [
        "In this homework assignment you will build a text classifier!\n",
        "\n",
        "We will use the data from the Kaggle contest: https://www.kaggle.com/competitions/nlp-getting-started/data From there you should download the train.csv file. We will divide it into training and test samples by the code below, you don't need to change it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFNXenqE_AoU"
      },
      "source": [
        "We will work with a dataset of twitter posts. We have to solve the problem of binary classification - to determine whether the tweet contains information about the real disaster/incident or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDnNNf_c_AoU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TILmOkZ0_AoU"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKI53BCP_AoV",
        "outputId": "4d1c2186-26f5-48f9-a19e-ea8b2f159f2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np0cPTIX_AoV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5HG1OZJ_AoV"
      },
      "source": [
        "## Task 1 (0.5 points)\n",
        "\n",
        "Print the information about the gaps in the data. If there are gaps, fill them in with a blank line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yChUpmKh_AoV",
        "outputId": "220cea76-3577-4144-fd01-0a3b9ee91e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Пропусков в столбцах train:\n",
            " id             0\n",
            "keyword       44\n",
            "location    1760\n",
            "text           0\n",
            "target         0\n",
            "dtype: int64\n",
            "Пропусков в столбцах test:\n",
            " id            0\n",
            "keyword      17\n",
            "location    773\n",
            "text          0\n",
            "target        0\n",
            "dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td></td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td></td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td></td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id      keyword               location  \\\n",
              "2644  3796  destruction                          \n",
              "2227  3185       deluge                          \n",
              "5448  7769       police                     UK   \n",
              "132    191   aftershock                          \n",
              "6845  9810       trauma  Montgomery County, MD   \n",
              "\n",
              "                                                   text  target  \n",
              "2644  So you have a new weapon that can cause un-ima...       1  \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
              "132   Aftershock back to school kick off was great. ...       0  \n",
              "6845  in response to trauma Children of Addicts deve...       0  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('Gaps in the columns train:\\n', train.isnull().sum())\n",
        "print('Gaps in the columns test:\\n', test.isnull().sum())\n",
        "\n",
        "train = train.fillna('')\n",
        "test = test.fillna('')\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uOkJEVC_AoW"
      },
      "source": [
        "## Assignment 2 (1 point)\n",
        "Let's take a little look at our data. Visualize (where explicitly asked for) or display information about the following:\n",
        "\n",
        "1. What is the distribution of classes in the training sample?\n",
        "2. Look at the \"keyword\" column - take the 10 most occurring values, draw a step diagram of the distribution of classes depending on the keyword value, draw conclusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvgMXOqA_AoW"
      },
      "outputs": [],
      "source": [
        "import matplotlib as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1WTsW6J_AoW",
        "outputId": "6b26c9e6-f3c4-418e-ed6e-5cb32e8625b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[<AxesSubplot:title={'center':'target'}>]]\n",
            "target\n",
            "0    3024\n",
            "1    2305\n",
            "Name: target, dtype: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(train.hist('target'))\n",
        "print(train.groupby('target')['target'].count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFp6oalx_AoW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpuasxFJ_AoW",
        "outputId": "65326cfb-0b92-4bb9-8893-febcbe64da94"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'matplotlib' has no attribute 'bar'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\LIVANO~1\\AppData\\Local\\Temp/ipykernel_2108/490429351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'keyword'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'keyword'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'target_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keyword'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'bar'"
          ]
        }
      ],
      "source": [
        "#For the training sample:\n",
        "frame = train.groupby(by = 'keyword', as_index = False)['target'].count().sort_values(by = 'target', ascending=False).head(10)\n",
        "frame['keyword']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fx0KcDr3_AoW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "a = pd.merge(frame, train, how='left', on='keyword')\n",
        "b = a.groupby(by = ['keyword', 'target_y'], as_index = False)['target_x'].count()\n",
        "groups = list(set(b['keyword']))\n",
        "zero = list(b.loc[(b.target_y == 0)]['target_x'])\n",
        "one = list(b.loc[(b.target_y == 1)]['target_x'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_c4g8jt_AoX"
      },
      "source": [
        "## Assignment 3 (0.5 points) \n",
        "\n",
        "This task asks you to combine all three text columns into one (just concatenate the rows) and remove the column with the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biC7uwbH_AoX"
      },
      "outputs": [],
      "source": [
        "train = train.drop(columns = 'id')\n",
        "test = test.drop(columns = 'id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7PJA9nG_AoX",
        "outputId": "c389efef-4bb5-40f2-c7de-5f00d90a3957"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>concat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1186</th>\n",
              "      <td>bridge%20collapse</td>\n",
              "      <td></td>\n",
              "      <td>Ashes 2015: AustraliaÛªs collapse at Trent Br...</td>\n",
              "      <td>0</td>\n",
              "      <td>bridge%20collapse  Ashes 2015: AustraliaÛªs c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4071</th>\n",
              "      <td>hail</td>\n",
              "      <td>Carol Stream, Illinois</td>\n",
              "      <td>GREAT MICHIGAN TECHNIQUE CAMP\\nB1G THANKS TO @...</td>\n",
              "      <td>1</td>\n",
              "      <td>hail Carol Stream, Illinois GREAT MICHIGAN TEC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5461</th>\n",
              "      <td>police</td>\n",
              "      <td>Houston</td>\n",
              "      <td>CNN: Tennessee movie theater shooting suspect ...</td>\n",
              "      <td>1</td>\n",
              "      <td>police Houston  CNN: Tennessee movie theater s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5787</th>\n",
              "      <td>rioting</td>\n",
              "      <td></td>\n",
              "      <td>Still rioting in a couple of hours left until ...</td>\n",
              "      <td>1</td>\n",
              "      <td>rioting  Still rioting in a couple of hours le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7445</th>\n",
              "      <td>wounds</td>\n",
              "      <td>Lake Highlands</td>\n",
              "      <td>Crack in the path where I wiped out this morni...</td>\n",
              "      <td>0</td>\n",
              "      <td>wounds Lake Highlands Crack in the path where ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5226</th>\n",
              "      <td>obliteration</td>\n",
              "      <td>Merica!</td>\n",
              "      <td>@Eganator2000 There aren't many Obliteration s...</td>\n",
              "      <td>0</td>\n",
              "      <td>obliteration Merica! @Eganator2000 There aren'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5390</th>\n",
              "      <td>panic</td>\n",
              "      <td></td>\n",
              "      <td>just had a panic attack bc I don't have enough...</td>\n",
              "      <td>0</td>\n",
              "      <td>panic  just had a panic attack bc I don't have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>blood</td>\n",
              "      <td></td>\n",
              "      <td>Omron HEM-712C Automatic Blood Pressure Monito...</td>\n",
              "      <td>0</td>\n",
              "      <td>blood  Omron HEM-712C Automatic Blood Pressure...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7603</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Officials say a quarantine is in place at an A...</td>\n",
              "      <td>1</td>\n",
              "      <td>Officials say a quarantine is in place at an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7270</th>\n",
              "      <td>whirlwind</td>\n",
              "      <td>Stamford &amp; Cork (&amp; Shropshire)</td>\n",
              "      <td>I moved to England five years ago today. What ...</td>\n",
              "      <td>1</td>\n",
              "      <td>whirlwind Stamford &amp; Cork (&amp; Shropshire) I mov...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5329 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                keyword                        location  \\\n",
              "1186  bridge%20collapse                                   \n",
              "4071               hail          Carol Stream, Illinois   \n",
              "5461             police                        Houston    \n",
              "5787            rioting                                   \n",
              "7445             wounds                  Lake Highlands   \n",
              "...                 ...                             ...   \n",
              "5226       obliteration                         Merica!   \n",
              "5390              panic                                   \n",
              "860               blood                                   \n",
              "7603                                                      \n",
              "7270          whirlwind  Stamford & Cork (& Shropshire)   \n",
              "\n",
              "                                                   text  target  \\\n",
              "1186  Ashes 2015: AustraliaÛªs collapse at Trent Br...       0   \n",
              "4071  GREAT MICHIGAN TECHNIQUE CAMP\\nB1G THANKS TO @...       1   \n",
              "5461  CNN: Tennessee movie theater shooting suspect ...       1   \n",
              "5787  Still rioting in a couple of hours left until ...       1   \n",
              "7445  Crack in the path where I wiped out this morni...       0   \n",
              "...                                                 ...     ...   \n",
              "5226  @Eganator2000 There aren't many Obliteration s...       0   \n",
              "5390  just had a panic attack bc I don't have enough...       0   \n",
              "860   Omron HEM-712C Automatic Blood Pressure Monito...       0   \n",
              "7603  Officials say a quarantine is in place at an A...       1   \n",
              "7270  I moved to England five years ago today. What ...       1   \n",
              "\n",
              "                                                 concat  \n",
              "1186  bridge%20collapse  Ashes 2015: AustraliaÛªs c...  \n",
              "4071  hail Carol Stream, Illinois GREAT MICHIGAN TEC...  \n",
              "5461  police Houston  CNN: Tennessee movie theater s...  \n",
              "5787  rioting  Still rioting in a couple of hours le...  \n",
              "7445  wounds Lake Highlands Crack in the path where ...  \n",
              "...                                                 ...  \n",
              "5226  obliteration Merica! @Eganator2000 There aren'...  \n",
              "5390  panic  just had a panic attack bc I don't have...  \n",
              "860   blood  Omron HEM-712C Automatic Blood Pressure...  \n",
              "7603    Officials say a quarantine is in place at an...  \n",
              "7270  whirlwind Stamford & Cork (& Shropshire) I mov...  \n",
              "\n",
              "[5329 rows x 5 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['concat'] = train['keyword'] + ' ' + train['location'] + ' ' + train['text']\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owOFWBqe_AoX",
        "outputId": "6cacb555-0815-424f-ae87-98c8da0973de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>concat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>destruction</td>\n",
              "      <td></td>\n",
              "      <td>So you have a new weapon that can cause un-ima...</td>\n",
              "      <td>1</td>\n",
              "      <td>destruction  So you have a new weapon that can...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>deluge</td>\n",
              "      <td></td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
              "      <td>0</td>\n",
              "      <td>deluge  The f$&amp;amp;@ing things I do for #GISHW...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
              "      <td>1</td>\n",
              "      <td>police UK DT @georgegalloway: RT @Galloway4May...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>aftershock</td>\n",
              "      <td></td>\n",
              "      <td>Aftershock back to school kick off was great. ...</td>\n",
              "      <td>0</td>\n",
              "      <td>aftershock  Aftershock back to school kick off...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts deve...</td>\n",
              "      <td>0</td>\n",
              "      <td>trauma Montgomery County, MD in response to tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4307</th>\n",
              "      <td>hellfire</td>\n",
              "      <td>570 Vanderbilt; Brooklyn, NY</td>\n",
              "      <td>New cocktail on the list! El Diablo Mas Verde:...</td>\n",
              "      <td>0</td>\n",
              "      <td>hellfire 570 Vanderbilt; Brooklyn, NY New cock...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3375</th>\n",
              "      <td>evacuation</td>\n",
              "      <td>USA</td>\n",
              "      <td>Bend Post Office roofers cut gas line prompt e...</td>\n",
              "      <td>1</td>\n",
              "      <td>evacuation USA Bend Post Office roofers cut ga...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1710</th>\n",
              "      <td>collided</td>\n",
              "      <td></td>\n",
              "      <td>Monsoon flooding - Monsoon rains have have hit...</td>\n",
              "      <td>1</td>\n",
              "      <td>collided  Monsoon flooding - Monsoon rains hav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4898</th>\n",
              "      <td>massacre</td>\n",
              "      <td>Ireland</td>\n",
              "      <td>Remember this was a massacre of civilians. #Hi...</td>\n",
              "      <td>1</td>\n",
              "      <td>massacre Ireland Remember this was a massacre ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6753</th>\n",
              "      <td>tornado</td>\n",
              "      <td>Asheville, NC</td>\n",
              "      <td>I liked a @YouTube video http://t.co/itnKBxgWL...</td>\n",
              "      <td>1</td>\n",
              "      <td>tornado Asheville, NC I liked a @YouTube video...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2284 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          keyword                      location  \\\n",
              "2644  destruction                                 \n",
              "2227       deluge                                 \n",
              "5448       police                            UK   \n",
              "132    aftershock                                 \n",
              "6845       trauma         Montgomery County, MD   \n",
              "...           ...                           ...   \n",
              "4307     hellfire  570 Vanderbilt; Brooklyn, NY   \n",
              "3375   evacuation                           USA   \n",
              "1710     collided                                 \n",
              "4898     massacre                       Ireland   \n",
              "6753      tornado                 Asheville, NC   \n",
              "\n",
              "                                                   text  target  \\\n",
              "2644  So you have a new weapon that can cause un-ima...       1   \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0   \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1   \n",
              "132   Aftershock back to school kick off was great. ...       0   \n",
              "6845  in response to trauma Children of Addicts deve...       0   \n",
              "...                                                 ...     ...   \n",
              "4307  New cocktail on the list! El Diablo Mas Verde:...       0   \n",
              "3375  Bend Post Office roofers cut gas line prompt e...       1   \n",
              "1710  Monsoon flooding - Monsoon rains have have hit...       1   \n",
              "4898  Remember this was a massacre of civilians. #Hi...       1   \n",
              "6753  I liked a @YouTube video http://t.co/itnKBxgWL...       1   \n",
              "\n",
              "                                                 concat  \n",
              "2644  destruction  So you have a new weapon that can...  \n",
              "2227  deluge  The f$&amp;@ing things I do for #GISHW...  \n",
              "5448  police UK DT @georgegalloway: RT @Galloway4May...  \n",
              "132   aftershock  Aftershock back to school kick off...  \n",
              "6845  trauma Montgomery County, MD in response to tr...  \n",
              "...                                                 ...  \n",
              "4307  hellfire 570 Vanderbilt; Brooklyn, NY New cock...  \n",
              "3375  evacuation USA Bend Post Office roofers cut ga...  \n",
              "1710  collided  Monsoon flooding - Monsoon rains hav...  \n",
              "4898  massacre Ireland Remember this was a massacre ...  \n",
              "6753  tornado Asheville, NC I liked a @YouTube video...  \n",
              "\n",
              "[2284 rows x 5 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test['concat'] = test['keyword'] + ' ' + test['location'] + ' ' + test['text']\n",
        "test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lOXBxlu_AoX"
      },
      "source": [
        "## Task 4 (0.5 points)\n",
        "\n",
        "Next we will work with only the train part for now.\n",
        "\n",
        "1. Preprocess the data (train part) using CountVectorizer.\n",
        "2. What is the size of the matrix?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnR7Y5OH_AoX",
        "outputId": "11ca018f-9234-49d1-9b84-7afedb2cb034"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\livanovskaya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkr7AExh_AoX"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b89TwXtb_AoX",
        "outputId": "178817cd-ad71-4bd0-b80c-c62c3e3e18d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер получившийся матрицы: (5329, 18455)\n"
          ]
        }
      ],
      "source": [
        "X = vectorizer.fit_transform(train[\"concat\"])\n",
        "print('Размер получившийся матрицы:', X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90JU6CX6_AoX"
      },
      "source": [
        "## Assignment 5 (1 point).\n",
        "\n",
        "In the previous paragraph you should have had a fairly large matrix.\n",
        "If you look at the text, you will see that there are a lot of special characters, references, and other garbage.\n",
        "\n",
        "Let's also take a look at the vocabulary that resulted from the CountVectorizer construction, it can be found in the vocabulary_ field of the instance of this class. Let's write a function that prints the answers to the following questions:\n",
        "\n",
        "1. Find all the words in this vocabulary that contain numbers. How many of these words did you find?\n",
        "\n",
        "2. Find all the words that contain punctuation symbols. How many such words did you find? \n",
        "\n",
        "3. How many hashtags (token starts with #) and mentions (token starts with @) left in the dictionary?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z0a8yUG_AoX",
        "outputId": "dff2b371-cd2a-45f7-dcc3-98661d590965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Слов с цифрами: 3812\n",
            "Слов со знаками пунктуации: 315\n",
            "Слов-хэштегов: 0\n",
            "Слов-упоминаний: 0\n"
          ]
        }
      ],
      "source": [
        "#vocabualary\n",
        "words = pd.Series(vectorizer.vocabulary_).sort_values().index\n",
        "from string import punctuation\n",
        "\n",
        "\n",
        "def clearing(words):\n",
        "    #Numbers\n",
        "    numbers = []\n",
        "    c = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "    for i in words:\n",
        "        for j in i:\n",
        "            if j in c:\n",
        "                numbers.append(i)\n",
        "                break\n",
        "    #punctuation\n",
        "    punc = []\n",
        "    a = punctuation\n",
        "    for i in words:\n",
        "        for j in i:\n",
        "            if j in a:\n",
        "                punc.append(i)\n",
        "                break\n",
        "    #hashtags\n",
        "    has = []\n",
        "    for i in words:\n",
        "        if i[0] == '#':\n",
        "                has.append(i)\n",
        "\n",
        "                \n",
        "    #mentions\n",
        "    tag = []\n",
        "    for i in words:\n",
        "        if i[0]=='@':\n",
        "                tag.append(i)\n",
        "\n",
        "    return [numbers, punc, has, tag]\n",
        "print('Words with Numbers:', len(clearing(words)[0]))\n",
        "print('Words with punctuations:', len(clearing(words)[1]))\n",
        "print('Word hashtags:', len(clearing(words)[2]))\n",
        "print('Words of mentions:', len(clearing(words)[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLcrEshO_AoY"
      },
      "source": [
        "## Задание 6 (0.5 балла)\n",
        "\n",
        "Вспомним, что на семинаре по текстам мы узнали, что в nltk есть специальный токенизатор для текстов - TweetTokenizer. Попробуем применить CountVectorizer с этим токенизатором. Ответьте на все вопросы из предыдущего пункта для TweetTokenizer и сравните результаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9Kukr6l_AoY",
        "outputId": "35acc963-cfaf-4701-bd12-72294092bfda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\livanovskaya\\anaconda3\\lib\\site-packages (3.6.5)\n",
            "Requirement already satisfied: click in c:\\users\\livanovskaya\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
            "Requirement already satisfied: joblib in c:\\users\\livanovskaya\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\livanovskaya\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\livanovskaya\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\livanovskaya\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k75nuOM9_AoY",
        "outputId": "14f64634-baf6-4e6e-b2e6-2b1a99724556"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class TweetTokenizer in module nltk.tokenize.casual:\n",
            "\n",
            "class TweetTokenizer(builtins.object)\n",
            " |  TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False, match_phone_numbers=True)\n",
            " |  \n",
            " |  Tokenizer for tweets.\n",
            " |  \n",
            " |      >>> from nltk.tokenize import TweetTokenizer\n",
            " |      >>> tknzr = TweetTokenizer()\n",
            " |      >>> s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
            " |      >>> tknzr.tokenize(s0)\n",
            " |      ['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3'\n",
            " |      , 'and', 'some', 'arrows', '<', '>', '->', '<--']\n",
            " |  \n",
            " |  Examples using `strip_handles` and `reduce_len parameters`:\n",
            " |  \n",
            " |      >>> tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
            " |      >>> s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
            " |      >>> tknzr.tokenize(s1)\n",
            " |      [':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, preserve_case=True, reduce_len=False, strip_handles=False, match_phone_numbers=True)\n",
            " |      Create a `TweetTokenizer` instance with settings for use in the `tokenize` method.\n",
            " |      \n",
            " |      :param preserve_case: Flag indicating whether to preserve the casing (capitalisation)\n",
            " |          of text used in the `tokenize` method. Defaults to True.\n",
            " |      :type preserve_case: bool\n",
            " |      :param reduce_len: Flag indicating whether to replace repeated character sequences\n",
            " |          of length 3 or greater with sequences of length 3. Defaults to False.\n",
            " |      :type reduce_len: bool\n",
            " |      :param strip_handles: Flag indicating whether to remove Twitter handles of text used\n",
            " |          in the `tokenize` method. Defaults to False.\n",
            " |      :type strip_handles: bool\n",
            " |      :param match_phone_numbers: Flag indicating whether the `tokenize` method should look\n",
            " |          for phone numbers. Defaults to True.\n",
            " |      :type match_phone_numbers: bool\n",
            " |  \n",
            " |  tokenize(self, text: str) -> List[str]\n",
            " |      Tokenize the input text.\n",
            " |      \n",
            " |      :param text: str\n",
            " |      :rtype: list(str)\n",
            " |      :return: a tokenized list of strings; joining this list returns        the original string if `preserve_case=False`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  PHONE_WORD_RE\n",
            " |      Secondary core TweetTokenizer regex\n",
            " |  \n",
            " |  WORD_RE\n",
            " |      Core TweetTokenizer regex\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "help(TweetTokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbieRhkm_AoY",
        "outputId": "4990cfbb-6095-4c7a-9363-036500fc2c00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5329, 19670)"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tw = TweetTokenizer()\n",
        "twvect = CountVectorizer(tokenizer = tw.tokenize) \n",
        "XX = twvect.fit_transform(train[\"concat\"])\n",
        "XX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua-g_kMT_AoY",
        "outputId": "ca865c4d-9485-4ac5-e5e7-4fbae2cece05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Слов с цифрами: 3939\n",
            "Слов со знаками пунктуации: 7338\n",
            "Слов-хэштегов: 1470\n",
            "Слов-упоминаний: 1679\n"
          ]
        }
      ],
      "source": [
        "words2 = pd.Series(twvect.vocabulary_).sort_values().index\n",
        "print('Words with Numbers:', len(clearing(words2)[0]))\n",
        "print('Words with punctuations:', len(clearing(words2)[1]))\n",
        "print('Word hashtags:', len(clearing(words2)[2]))\n",
        "print('Words of mentions:', len(clearing(words2)[3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pm0MRwVe_AoY",
        "outputId": "0afc091a-295b-496e-e4e1-fda97c6d834b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TweetTokenizer takes hashtags and mentions into account, so it has no separation of characters and words in these situations. Because of this, it was able to highlight and count hashtags and mentions (while the regular tokenizer couldnt - so there values are 0). In addition, the regular tokenizer separates punctuation marks from words and so compared to the tweet tokenizer, which separates emoticons and leaves the grid and doggie with the word, the tweet tokenizer gets many more unique words with punctuation marks (and the regular tokenizer counts the punctuation mark as a separate word and so all words that had those marks are reduced to the word mark. Therefore, there are far fewer words with punctuation marks.)\n"
          ]
        }
      ],
      "source": [
        "#comparison:\n",
        "print('TweetTokenizer takes hashtags and mentions into account, so it has no separation of characters and words in these situations. Because of this, it was able to highlight and count hashtags and mentions (while the regular tokenizer couldnt - so there values are 0). In addition, the regular tokenizer separates punctuation marks from words and so compared to the tweet tokenizer, which separates emoticons and leaves the grid and doggie with the word, the tweet tokenizer gets many more unique words with punctuation marks (and the regular tokenizer counts the punctuation mark as a separate word and so all words that had those marks are reduced to the word mark. Therefore, there are far fewer words with punctuation marks.)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX_yc4Jz_AoY"
      },
      "source": [
        "## Task 7 (2 points).\n",
        "\n",
        "In scikit-learn we can evaluate the matrix counting process through CountVectorizer. CountVectorizer, like other \\_VectorizerMixin heirs, has an argument tokenizer and preprocessor. The preprocessor will apply at the beginning to each row of your dataset, the tokenizer should accept the row and return the tokens.\n",
        "Let's write a custom tokenizer that does what we need: \n",
        "\n",
        "0. Makes all the letters lowercase\n",
        "1. Split the text into tokens using the TweetTokenizer from the nltk package\n",
        "2. It will remove all the tokens containing non Latin letters except for smiley faces (we will consider tokens containing only punctuation and at least one bracket) and hashtags which contain only Latin letters after the initial #.\n",
        "3. Deletes all the tokens listed in nltk.corpus.stopwords.words('english')\n",
        "4. Stemming with SnowballStemmer.\n",
        "\n",
        "Demonstrate how your function works on the first ten texts in the training sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78BzofAE_AoY",
        "outputId": "d0a5c944-4e56-4a45-b646-faca25582b32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\livanovskaya\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmbkEsRQ_AoY"
      },
      "outputs": [],
      "source": [
        "def Custom_tokenizer(data):\n",
        "    from nltk.tokenize import TweetTokenizer\n",
        "    from string import punctuation\n",
        "    from nltk.stem.snowball import SnowballStemmer\n",
        "    from nltk.corpus import stopwords\n",
        "    data = data.lower()\n",
        "    tw = TweetTokenizer()\n",
        "    words = tw.tokenize(data) #array of strings\n",
        "    allowed = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    punc = punctuation\n",
        "    stop_words = []\n",
        "    for i in words:\n",
        "        for j in i:\n",
        "            if j not in allowed:\n",
        "                if i[0] != '#':\n",
        "                    if j in punc:\n",
        "                        if ('(' not in i) and (')' not in i):\n",
        "                            stop_words.append(i)\n",
        "                            break\n",
        "                    else:\n",
        "                        stop_words.append(i)\n",
        "                        break\n",
        "                        \n",
        "    english_stop = nltk.corpus.stopwords.words('english')\n",
        "    remove = [i for i in words if not i in stop_words and i not in english_stop]\n",
        "    \n",
        "    #stemming \n",
        "    stemmer = SnowballStemmer('english')\n",
        "    text_stemmed = [stemmer.stem(w) for w in remove]\n",
        "    return text_stemmed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivmGu-iQ_AoZ",
        "outputId": "a2b65065-428f-4b97-f464-2e6fad95ef87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['#goblu', '#mlb', '#wrestleon', '(', ')', 'accid', 'air', 'airplan',\n",
              "       'among', 'ash', 'ask', 'australia', 'beach', 'begin', 'bloodi', 'boast',\n",
              "       'bridg', 'bundl', 'burn', 'came', 'camp', 'carol', 'citi', 'class',\n",
              "       'cnn', 'collaps', 'coupl', 'crack', 'dead', 'debri', 'destroy', 'elbow',\n",
              "       'england', 'examin', 'except', 'exchang', 'expert', 'found', 'franc',\n",
              "       'french', 'fun', 'great', 'hail', 'highland', 'histori', 'hour', 'hous',\n",
              "       'houston', 'idk', 'illinoi', 'indian', 'island', 'isol', 'kill',\n",
              "       'killer', 'knee', 'lake', 'left', 'maracay', 'michigan', 'morn', 'movi',\n",
              "       'nirgua', 'offic', 'pakistani', 'path', 'perth', 'polic', 'realli',\n",
              "       'remorseless', 'reunion', 'right', 'riot', 'run', 'shoot', 'shot',\n",
              "       'show', 'smirk', 'somewher', 'still', 'stream', 'surfac', 'suspect',\n",
              "       'techniqu', 'tennesse', 'thank', 'theater', 'trent', 'venezuela',\n",
              "       'video', 'wipe', 'world', 'worst', 'wound'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "twvec = CountVectorizer(tokenizer = Custom_tokenizer) \n",
        "XXX = twvec.fit_transform(train['concat'].head(10))\n",
        "words3 = pd.Series(twvec.vocabulary_).sort_values().index\n",
        "words3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "731G0f_B_AoZ"
      },
      "source": [
        "## Assignment 8 (1 point)\n",
        "\n",
        "1. Apply CountVectorizer with the tokenizer implemented above to the training and test samples.\n",
        "2. Train LogisticRegression on the obtained features.\n",
        "3. Calculate the f1-score metric on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXiGcq3f_AoZ"
      },
      "outputs": [],
      "source": [
        "train_1 = CountVectorizer(tokenizer = Custom_tokenizer) \n",
        "X_tr = train_1.fit_transform(train['concat'])\n",
        "Y_tr = train['target']\n",
        "X_te = train_1.transform(test['concat'])\n",
        "Y_te = test['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os2fweSL_AoZ",
        "outputId": "302956ce-717a-4d00-fe46-393b7ea971d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_tr, Y_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKpdyVc5_AoZ",
        "outputId": "55a3c4d2-bd5e-4556-dfba-f21a8852d5fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.754433100483611\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "preds_te = model.predict(X_te)\n",
        "print(f1_score(Y_te, preds_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MkcrLe1_Aoh"
      },
      "source": [
        "## Assignment 9 (1 point)\n",
        "\n",
        "1. Repeat task 7(8), but with the tf-idf vectorizer. How has the quality changed?\n",
        "2. We can reduce the size of our matrix even further if we discard values of df close to one. Most likely, such words do not carry much information about the category, as they occur quite often. Limit the maximum df in TfIdfVectorizer parameters, set the upper bound equal to 0.9. How has the matrix size changed, how has the quality changed?\n",
        "We can also reduce the size of the matrix by removing words with too small df. Were we able to achieve an improvement in quality? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-nlnwIo_Aoi"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5aUyApz_Aoi"
      },
      "outputs": [],
      "source": [
        "train_2 = TfidfVectorizer(tokenizer = Custom_tokenizer) \n",
        "X_tr2 = train_2.fit_transform(train['concat'])\n",
        "Y_tr2 = train['target']\n",
        "X_te2 = train_2.transform(test['concat'])\n",
        "Y_te2 = test['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXhvAe1T_Aoj",
        "outputId": "5c276564-f7c3-4bec-e026-5258255a6535"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5329, 10607)"
            ]
          },
          "execution_count": 196,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_tr2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2BdQFS7_Aoj",
        "outputId": "f28d6c6c-ceca-4e11-a74d-d8e295c9df53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 189,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2 = LogisticRegression()\n",
        "model2.fit(X_tr2, Y_tr2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B91uDFil_Aoj",
        "outputId": "1afd9185-57c8-4231-a80f-77622777a4db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7422222222222221\n"
          ]
        }
      ],
      "source": [
        "preds_te2 = model2.predict(X_te2)\n",
        "print(f1_score(Y_te2, preds_te2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "id": "HL9zWd3q_Aoj",
        "outputId": "a966f79d-a5e6-4b87-f5c3-7ce225256cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quality has decreased slightly\n"
          ]
        }
      ],
      "source": [
        "print('The quality has decreased slightly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNPB8Zk4_Aoj"
      },
      "source": [
        "Изменяем df:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fNQTNh8_Aoj",
        "outputId": "5c22f898-74de-4663-96d5-e4ef86158014"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5329, 10607)"
            ]
          },
          "execution_count": 199,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_3 = TfidfVectorizer(tokenizer = Custom_tokenizer, max_df=0.9) \n",
        "X_tr3 = train_3.fit_transform(train['concat'])\n",
        "Y_tr3 = train['target']\n",
        "X_te3 = train_3.transform(test['concat'])\n",
        "Y_te3 = test['target']\n",
        "X_tr3.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eutkQ_N2_Aoj"
      },
      "source": [
        "the size of the matrix has not changed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYbGPfH__Aoj",
        "outputId": "48d1b4b6-26d4-4a7f-f3cc-73f05344e1fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7422222222222221\n"
          ]
        }
      ],
      "source": [
        "model3 = LogisticRegression()\n",
        "model3.fit(X_tr3, Y_tr3)\n",
        "preds_te3 = model3.predict(X_te3)\n",
        "print(f1_score(Y_te3, preds_te3))"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "dgmgffHZ_Aok"
      },
      "source": [
        "The quality did not change either.\n",
        "Let's see what changes if we remove words with a small df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4li_USp_Aok",
        "outputId": "eac2de3f-61cc-44dc-a2fd-45cb11affc00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5329, 4534)"
            ]
          },
          "execution_count": 207,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_3 = TfidfVectorizer(tokenizer = Custom_tokenizer, min_df=0.0003) \n",
        "X_tr3 = train_3.fit_transform(train['concat'])\n",
        "Y_tr3 = train['target']\n",
        "X_te3 = train_3.transform(test['concat'])\n",
        "Y_te3 = test['target']\n",
        "X_tr3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQji6VQI_Aok",
        "outputId": "3fe76bd0-63c0-4178-c3bd-0dbf40dfad63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7462520821765685\n"
          ]
        }
      ],
      "source": [
        "model3 = LogisticRegression()\n",
        "model3.fit(X_tr3, Y_tr3)\n",
        "preds_te3 = model3.predict(X_te3)\n",
        "print(f1_score(Y_te3, preds_te3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRcpRyZV_Aok"
      },
      "source": [
        "When the lower threshold df is increased, the number of words decreased (matrix dimension), while the quality measured by the f1 measure increased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JgV002L_Aok"
      },
      "source": [
        "## Assignment 10 (1 point).\n",
        "\n",
        "Another popular trick for reducing the number of features is called the hashing trick. The idea is that we randomly group the features and ..... add them up! And then we remove the original features. In the end, all of our traits are just sums of the original traits. It sounds strange, but it works just fine. Let's test this trick in our setting.\n",
        "Also with this approach you don't need to store the token->index dictionary, which is also sometimes useful.\n",
        "\n",
        "1. Repeat task 7(8) with HashingVectorizer, specify the number of features equal to 5000.\n",
        "2. Which approach has the highest score?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dljopjw_Aok"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtOocL1W_Aok",
        "outputId": "22914d23-f1bf-4f85-ce66-c591ead3531e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5329, 5000)"
            ]
          },
          "execution_count": 211,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_3 = HashingVectorizer(tokenizer = Custom_tokenizer, n_features=5000) \n",
        "X_tr3 = train_3.fit_transform(train['concat'])\n",
        "#words7 = pd.Series(train_3.vocabulary_).sort_values().index\n",
        "Y_tr3 = train['target']\n",
        "X_te3 = train_3.transform(test['concat'])\n",
        "Y_te3 = test['target']\n",
        "X_tr3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ywSXsXF_Aok",
        "outputId": "89c0d297-9553-4c76-9cac-fac355ad4b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7191513121161363\n"
          ]
        }
      ],
      "source": [
        "model3 = LogisticRegression()\n",
        "model3.fit(X_tr3, Y_tr3)\n",
        "preds_te3 = model3.predict(X_te3)\n",
        "print(f1_score(Y_te3, preds_te3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J-IJjG9_Aol"
      },
      "source": [
        "Answer: Of all the approaches that were considered, CountVectorizer with custom tokenizer gave the highest result because it gave the highest f1 measure, while the other vectorizers trained the model with lower quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5haoAJ5_Aol"
      },
      "source": [
        "## Assignment 11 (1 point).\n",
        "\n",
        "In this task you need to achieve an f1 measure of at least 0.75 on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-lUc1i9_Aol",
        "outputId": "100ea0f8-68ab-4883-97f6-7e6e5f88d892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.754433100483611\n"
          ]
        }
      ],
      "source": [
        "train_1 = CountVectorizer(tokenizer = Custom_tokenizer) \n",
        "X_tr = train_1.fit_transform(train['concat'])\n",
        "Y_tr = train['target']\n",
        "X_te = train_1.transform(test['concat'])\n",
        "Y_te = test['target']\n",
        "model = LogisticRegression()\n",
        "model.fit(X_tr, Y_tr)\n",
        "preds_te = model.predict(X_te)\n",
        "print(f1_score(Y_te, preds_te))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}